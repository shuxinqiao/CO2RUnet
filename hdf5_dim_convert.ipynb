{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conver HDF5 file from (H,W,N,C) into (N,C,H,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/200000\n",
      "4000/200000\n",
      "6000/200000\n",
      "8000/200000\n",
      "10000/200000\n",
      "12000/200000\n",
      "14000/200000\n",
      "16000/200000\n",
      "18000/200000\n",
      "20000/200000\n",
      "22000/200000\n",
      "24000/200000\n",
      "26000/200000\n",
      "28000/200000\n",
      "30000/200000\n",
      "32000/200000\n",
      "34000/200000\n",
      "36000/200000\n",
      "38000/200000\n",
      "40000/200000\n",
      "42000/200000\n",
      "44000/200000\n",
      "46000/200000\n",
      "48000/200000\n",
      "50000/200000\n",
      "52000/200000\n",
      "54000/200000\n",
      "56000/200000\n",
      "58000/200000\n",
      "60000/200000\n",
      "62000/200000\n",
      "64000/200000\n",
      "66000/200000\n",
      "68000/200000\n",
      "70000/200000\n",
      "72000/200000\n",
      "74000/200000\n",
      "76000/200000\n",
      "78000/200000\n",
      "80000/200000\n",
      "82000/200000\n",
      "84000/200000\n",
      "86000/200000\n",
      "88000/200000\n",
      "90000/200000\n",
      "92000/200000\n",
      "94000/200000\n",
      "96000/200000\n",
      "98000/200000\n",
      "100000/200000\n",
      "102000/200000\n",
      "104000/200000\n",
      "106000/200000\n",
      "108000/200000\n",
      "110000/200000\n",
      "112000/200000\n",
      "114000/200000\n",
      "116000/200000\n",
      "118000/200000\n",
      "120000/200000\n",
      "122000/200000\n",
      "124000/200000\n",
      "126000/200000\n",
      "128000/200000\n",
      "130000/200000\n",
      "132000/200000\n",
      "134000/200000\n",
      "136000/200000\n",
      "138000/200000\n",
      "140000/200000\n",
      "142000/200000\n",
      "144000/200000\n",
      "146000/200000\n",
      "148000/200000\n",
      "150000/200000\n",
      "152000/200000\n",
      "154000/200000\n",
      "156000/200000\n",
      "158000/200000\n",
      "160000/200000\n",
      "162000/200000\n",
      "164000/200000\n",
      "166000/200000\n",
      "168000/200000\n",
      "170000/200000\n",
      "172000/200000\n",
      "174000/200000\n",
      "176000/200000\n",
      "178000/200000\n",
      "180000/200000\n",
      "182000/200000\n",
      "184000/200000\n",
      "186000/200000\n",
      "188000/200000\n",
      "190000/200000\n",
      "192000/200000\n",
      "194000/200000\n",
      "196000/200000\n",
      "198000/200000\n",
      "200000/200000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "data_dir = './'\n",
    "\n",
    "input_file_path = data_dir + 'train_200000.hdf5'\n",
    "output_file_path = data_dir + 'train_200000_reshaped.hdf5'\n",
    "\n",
    "\n",
    "# Open the existing HDF5 file for reading\n",
    "with h5py.File(input_file_path, 'r') as hf_r:\n",
    "    # Get the existing datasets\n",
    "    train_x = hf_r['k']\n",
    "    train_y = hf_r['S']\n",
    "    \n",
    "    # Get the dimensions of the datasets\n",
    "    H, W, N, _ = train_x.shape\n",
    "\n",
    "    total = 0\n",
    "    \n",
    "    # Create a temporary HDF5 file for writing\n",
    "    with h5py.File(output_file_path, 'w', libver='latest') as hf_w:\n",
    "        # Create new datasets with the desired dimensions and chunking\n",
    "        chunk_size = 2000  # Adjust the chunk size based on your available memory\n",
    "        hf_w.create_dataset('k', shape=(N, 3, H, W), dtype=train_x.dtype, chunks=(1, 3, H, W))\n",
    "        hf_w.create_dataset('S', shape=(N, 1, H, W), dtype=train_y.dtype, chunks=(1, 1, H, W))\n",
    "        \n",
    "        # Iterate through chunks and transpose data\n",
    "        for start in range(0, N, chunk_size):\n",
    "            end = min(start + chunk_size, N)\n",
    "            chunk_x = train_x[:, :, start:end, :].transpose((2, 3, 0, 1))\n",
    "            chunk_y = train_y[:, :, start:end, :].transpose((2, 3, 0, 1))\n",
    "            \n",
    "            # Write the transposed chunk to the new datasets\n",
    "            hf_w['k'][start:end, :, :, :] = chunk_x\n",
    "            hf_w['S'][start:end, :, :, :] = chunk_y\n",
    "\n",
    "            total += chunk_size\n",
    "            print(str(total) + \"/\" + str(N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of orginial and transposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of HDF5 file: ./train_200000_reshaped.hdf5\n",
      "\n",
      "Datasets:\n",
      "S - Shape: (200000, 1, 128, 128), Dtype: float64, Compression: None, Compression Options: None\n",
      "k - Shape: (200000, 3, 128, 128), Dtype: float64, Compression: None, Compression Options: None\n",
      "---------\n",
      "Summary of HDF5 file: ./train_200000.hdf5\n",
      "\n",
      "Datasets:\n",
      "S - Shape: (128, 128, 200000, 1), Dtype: float64, Compression: None, Compression Options: None\n",
      "k - Shape: (128, 128, 200000, 3), Dtype: float64, Compression: None, Compression Options: None\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "data_dir = './'\n",
    "\n",
    "def print_hdf5_summary(file_path):\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        print(\"Summary of HDF5 file:\", file_path)\n",
    "        print(\"\")\n",
    "\n",
    "        # Print information about datasets\n",
    "        print(\"Datasets:\")\n",
    "        hf.visititems(print_dataset_info)\n",
    "        print(\"---------\")\n",
    "\n",
    "def print_dataset_info(name, obj):\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{name} - Shape: {obj.shape}, Dtype: {obj.dtype}, Compression: {obj.compression}, Compression Options: {obj.compression_opts}\")\n",
    "\n",
    "# Provide the path to your HDF5 file\n",
    "file_path = data_dir + 'train_200000_reshaped.hdf5'\n",
    "\n",
    "# Print the summary of the HDF5 file\n",
    "print_hdf5_summary(file_path)\n",
    "\n",
    "# Provide the path to your HDF5 file\n",
    "file_path = data_dir + 'train_200000.hdf5'\n",
    "\n",
    "# Print the summary of the HDF5 file\n",
    "print_hdf5_summary(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read certain line of data and verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3) float64\n",
      "(3, 128, 128) float64\n",
      "Same!\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def extract_data_from_two_datasets(file1, file2, target_index):\n",
    "    with h5py.File(file1, 'r') as infile1, h5py.File(file2, 'r') as infile2:\n",
    "        # Assuming the datasets have the same structure\n",
    "        dataset_shape = infile1['k'].shape\n",
    "\n",
    "        # Extract data from the first file\n",
    "        data1 = infile1['k'][:, :, target_index, :]\n",
    "\n",
    "        # Extract data from the second file\n",
    "        data2 = infile2['k'][target_index, :, :, :]\n",
    "\n",
    "        # Write data to the output dataset\n",
    "        print(data1.shape, data1.dtype)\n",
    "        print(data2.shape, data2.dtype)\n",
    "\n",
    "        transpose = np.transpose(data1, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        comparison_result = np.array_equal(data2, transpose)\n",
    "        if not comparison_result:\n",
    "            diff_locations = np.where(transpose != data2)\n",
    "            print(f\"Differences found at positions: {diff_locations}\")\n",
    "        else:\n",
    "            print(\"Same!\")\n",
    "\n",
    "\n",
    "extract_data_from_two_datasets('train_200000.hdf5', 'train_200000_reshaped.hdf5', target_index=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
